/*

# Append-Only ZFS Backups

The goal of this configuration is to have encrypted, (multi-level) remote-replicated ZFS snapshots that can't be poisoned by a compromised origin.
This should provide snapshots (protecting against accidental or intentional (content) data corruption) and off-site backups (protecting against hardware failure or loss) in a single system, and be resilient against any failure or attack of/on the origin host (unless there is a ZFS permission exploit that can propagate via send streams).

The top requirement to realize this, regardless of whether streams are pushed or pulled, is that the receiver on the backup target does not have the ZFS `destroy` permission.
Building a replication scheme where the sender can't tell the target to roll back past the targets currently most recent snapshot, however, is quite challenging, as there are various situations in which ZFS send/receives can get stuck, especially if the operation got interrupted and not resumed soon enough afterwards, while new snapshots are created and old ones pruned in the meantime.

What additionally makes things difficult is that some edge cases are unlikely or infrequent to occur.
The turn-around time between noticing, reproducing, fixing and then deciding that the fix actually works is quite long.
That said, the current configuration has worked largely as it now since early 2023, in a network with several mobile backup sources (partially with long offline times), some always-online devices, and a single direct backup target that forwards all backups to two off-site sinks.
The backups have proven functional in restoring one of the mobile sources after a hardware failure, and in re-deploying multiple of the other systems on new hardware.


## Premise

ZFS snapshot replication is done by executing a coordinated pair of `send` and `receive` commands on the source and target, respectively, and streaming the output of the former into the latter.
Effectively, both commands are usually generated by one of the two hosts involved, and the commend on the other host is executed via SSH, which is also used to transfer the data stream.
Depending on who initiates the operation, this results in either a pull or push replication.

In principle one could probably design a similar architecture based on pull operations, but the setup here uses push replication.

The safety and security requirements from above impose some constraints on the ZFS operations that can be used:
* The receiving user can not have the `destroy` permission, as otherwise a compromised backup source could send a stream (or directly run a command) that deletes previous (un-compromised) snapshots, which poorly chosen commands could even propagate to multi-level / off-site backups.
    * Therefore, the sender can not initiate the deletion of any "sync snaps" (snapshots explicitly created to mark the point of the last successful sync).
* When forwarding datasets, no new snapshots (sync snaps) may be created by the sender, since the new snapshots would prevent receiving further snapshots from the original source.

The current implementations also requires all backed-up datasets to be encrypted, and only sends raw, (still) encrypted, streams, ensuring that the backup hosts never see any unencrypted (content) data.
Securing the encryptions keys against theft or loss is out of the scope here.


## Implementation

This configuration (currently) uses `sanoid` to create periodic snapshots on the original source, and to prune them independently on the source and all intermediate and final backup targets.
A [patched version](../../../overlays/sanoid.nix.md) of `syncoid` is used to periodically send the snapshots between SSH users with limited ZFS and unix permissions: the sending system only has an SSH key for an account on the backup system that can only execute sanoid commands, and those commands are restricted by the ZFS permissions that account has. Essentially, it can only append snapshots to its designated datasets.
This is complimented by services on the backup target hosts that periodically delete old sync snaps and, if a sync got stuck, do limited rollback (which both requires the dangerous `delete` permission).

Not using sync snaps, and therefore relying on the periodic snapshots as common sync points, is impractical if there is any chance that either the sender or the receiver may be offline for longer than the lifetime of the chosen snapshot on the other end.
With the patched in `--keep-sync-snap-target`, Syncoid therefore creates sync snaps and automatically only keeps the most recent (and successfully sent) one (per target) on the sender's side, but leaves the sync snaps on the target untouched (since it would fail at deleting anything there).

On the backup receiving end, no snapshots are ever created, but the received snapshots are pruned. Since the backup prune policy may be different from the sender's snapshot creation policy, it is thus quite possible that some intermediate snapshots are pruned right after having been received.
A separate service periodically deletes all but the most recent sync snap, as identified by the snapshots' names. This works because only successfully received snaps are listable, and since each backup only ever has one source.

Apparently (and it would be nice to have confirmed that this is documented, stable behavior), when instructed to keep X snapshots of any period Y, then Sanoid actually keeps any number of Y labeled snapshots created in the past X*Y.
This is necessary, as a compromised source would otherwise be able to send many cleared out snapshots with the various labeled and wait for Sanoid to prune the (not actually yet expired) snapshots with the actual data in them.

The worst an attacker owning the sending side/key can thus do is block any future backups, either by pushing any out-of-series snapshot, or filling the backup pool or quota.
(TODO: It would be nice to have this verified by someone who knows Linux/ZFS well.)


### Backup Forwarding

If a host is forwarding backups, that is, it is receiving some upstream host(s)' backups, and is also sending those same datasets to some other downstream host(s), then it is important that the forwarding host does not modify the backup datasets, i.e. it must send with `--no-sync-snap`, else future receiving would fail.
But when using `--no-sync-snap`, and not allowing any rollbacks on the send target, it is crucial to make sure that, for each send target, the last snapshot that was sent to it is not deleted (/pruned) on the forwarding host before the next send. Should that ever happen, then a manual rollback at the final target is required.

To ensure this, forwarder sets a property that lists all its direct send targets on the dataset.
The snapshots will inherit this property, and after successfully sending a snapshot to a target, that respective target will be removed from the property on that snapshot.
The forwarder can then locally delete any sync snaps that are older than the newest one which has been forwarded to all destinations.


## Automatic Backup Restoration during Installation

When using the [`nixos-installer`](https://github.com/NiklasGollenstede/nixos-installer/), if the argument `--zfs-restore` is provided during installation, then [`restore-zfs-backups`](./utils/restore-zfs-backups.sh), will automatically restore any backups specified for the host from one of their backup locations (`config.my.services.zfs.send.locations`).


## Monitoring

TODO (both concept and implementation)


## Example Host Config

A full, multi-host example with forwarding can be found in [`hosts/zfs-backup.nix.md`](../../../hosts/zfs-backup.nix.md).

Here is a short example with a single source and two direct sinks:
```nix
let ## Backups
    rpool = lib.my.rpoolOf config.networking.hostName;
in {
    ## On host "server"
    # Send/back-up the dataset "rpool/remote" ...
    wip.services.zfs.send.datasets = { "rpool/remote".targets = {
        # ... to "backup/server/remote" on the two hosts "backup-01"/"backup-02":
        backup-01.path = "backup/"; backup-02.path = "backup/";
    }; };

    ## On host "backup-01"/"backup-02" ...
    # Allow backups to be sent to the dataset(/pool) "backup" ...
    wip.services.zfs.receive.dataset = "backup";
    # ... from the hosts "server" and "laptop" ...
    wip.services.zfs.receive.sources = { "server" = { }; "laptop" = { }; };
    # ... and allow login using the keys ".../backup@${host}.pub" per host.
    wip.services.zfs.receive.getSshKey = host: lib.fileContents ("${.../.}/backup@${host}.pub");

    # Also do something that does local snapshotting and pruning on rpool/remote.
    # If you use sanoid on the source, consider this on the backup hosts:
    wip.services.zfs.receive.enableSanoid = true;
}
```
